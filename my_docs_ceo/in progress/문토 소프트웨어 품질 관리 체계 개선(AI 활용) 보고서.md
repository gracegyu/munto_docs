# **문토 소프트웨어 품질 관리 체계 개선(AI 활용) 보고서**

# 1. 서론

문토는 소셜 모임 플랫폼을 개발 및 운영하는 스타트업으로, 현재 Backend(Node.js), Web(React), App(Flutter) 세 영역에서 서비스를 제공하고 있습니다. 기존 개발 프로세스는 스펙 문서 없이 개발이 진행되며, 체계적인 테스트 없이 릴리즈 이후 발생하는 버그를 개발자와 기획자가 수동으로 수정하는 방식으로 운영되어 왔습니다. 이와 같은 운영 방식은 **개발자의 피로도 증가, 고객의 불만, 그리고 회사의 자원 낭비**로 이어지고 있습니다.

이 보고서는 문토의 품질 수준을 진단하고, 체계적인 품질 관리 체계를 정립하여 다음 단계의 품질 수준으로 도약하기 위한 방안을 제시합니다.

문토는 소셜 모임 플랫폼을 개발 및 운영하는 스타트업으로, 현재 Backend(Node.js), Web(React), App(Flutter) 세 영역에서 서비스를 제공하고 있습니다. 기존 개발 프로세스는 스펙 문서 없이 개발이 진행되며, 체계적인 테스트 없이 릴리즈 이후 발생하는 버그를 개발자와 기획자가 수동으로 수정하는 방식으로 운영되어 왔습니다. 이와 같은 운영 방식은 개발자의 피로도 증가, 고객의 불만, 그리고 회사의 자원 낭비로 이어지고 있습니다.

실제 사례로, 2025년 6월 13일 VOD 서비스 Web 프론트엔드 정식 배포를 앞두고 있었으나, 실서버에 붙여 테스트해본 결과 초기 동영상 등록부터 치명적인 버그가 발생하여 배포가 전면 보류되었습니다. 이는 개발자가 일부 기능만 랜덤으로 테스트했고, 전체 기능 테스트의 약 3% 수준만 검증된 상태에서 릴리즈를 시도했기 때문입니다. 결과적으로 버그 6건이 즉시 발견되었고, 잠재적으로 수십 건의 문제가 숨어 있을 가능성도 있으며, 이러한 이슈는 실제 고객의 사용 중에 노출될 수밖에 없습니다.

이와 같은 사례는 **Level 1의 품질 체계**를 가진 조직에서 흔히 발생하는 문제입니다. **테스트 계획 부재, 반복 불가능한 테스트 수행, 릴리즈 전 통합 테스트 생략 등은 결국 품질 비용을 고객과 회사가 함께 떠안는 구조**를 낳습니다. 이러한 리스크를 줄이고 **경쟁력을 확보하기 위해서는 명확한 테스트 체계와 품질 문화의 도입이 절실**합니다.

---

# 2. 소프트웨어 품질 수준의 일반적 정의

소프트웨어 품질 관리 수준은 다음과 같이 레벨링 할 수 있습니다. 이는 CMMI(Capability Maturity Model Integration)나 TMMi(Test Maturity Model Integration) 등의 모델을 참고하여 일반화한 단계입니다.

- **Level 1 – 초기(Ad-hoc, 주먹구구)**
  - 프로세스가 정의되지 않음
  - 스펙 없음, 문서 없음, 기록 없음
  - 개발자 개인 역량에 의존
  - 테스트는 수동으로, 릴리즈 후 즉각적인 수정 중심
  - 예시:
    - 초기 스타트업 (Seed 단계)
    - 1~5인 조직, 기획 없이 바로 구현 → 배포 → 사용자 피드백 기반 수정
    - 예: 창업 직후의 사이드 프로젝트, 혹은 첫 번째 MVP만 빠르게 내놓는 소규모 팀
- **Level 2 – 반복 가능(Repeatable)**
  - SRS(요구사항 명세서)와 일정 관리(WBS) 존재
  - Jira 등 이슈 관리 시스템 도입
  - 기본적인 형상 관리 및 버전 관리 체계 존재
  - 테스트 계획은 일부 존재하나, 이를 전담하여 조직이 없음
  - 예시:
    - 초기 품질 체계를 갖춘 스타트업이 QA 조직 없이 프로세스를 정비해가는 과도기적 단계
    - 일정/이슈 관리는 Jira를 기반으로 문서화 및 반복성을 확보하는 중
    - 예: 시리즈 A~B 단계에서 성장을 준비하며 품질 체계 정립을 시작한 조직
    - QA 전담자는 없지만, 명확한 품질 목표를 세우고 테스트 자동화를 준비하는 10명 내외 규모의 기술 스타트업
- **Level 3 – 정의됨(Defined)**
  - QA 조직 혹은 전담 테스트 팀 존재
  - 테스트 케이스, 테스트 시나리오 체계화
  - 정적 분석 및 CI/CD 통합 테스트 자동화 일부 수행
  - 릴리즈 기준 정의 (ex. 버그 제로, 커버리지 등)
  - **유닛 테스트 및 E2E 테스트 일부 도입**
  - 예시:
    - 초기 토스, 당근마켓 등 성장을 준비하는 스타트업의 플랫폼 팀
    - 10~30인 규모의 제품 조직이 있는 중견 스타트업
- **Level 4 – 관리됨(Managed)**
  - 정량적 품질 목표 설정 및 측정
  - 품질 메트릭 관리 (버그 밀도, 릴리즈 안정성 등)
  - 리스크 기반 테스트, 성능 테스트 등 확장
  - **유닛 테스트 커버리지 기준 수립, E2E 테스트 자동화 정착**
  - 예시:
    - 네이버, 카카오의 중간 규모 서비스팀
    - 토스, 배달의민족 등 주요 기술기업의 핵심 서비스팀
    - 삼성전자 소프트웨어 센터 산하 검증팀 등
- **Level 5 – 최적화됨(Optimizing)**
  - 지속적 개선 기반의 품질 관리
  - AI 기반 테스트 자동화, 예측적 분석 도입
  - 실사용자 행동 기반 품질 피드백 시스템 구축
  - 예시:
    - Google, Microsoft, Meta의 개발 인프라팀
    - Amazon의 DevOps/Quality Engineering 팀
    - NASA, Intel, Tesla 등의 품질 최우선 조직 (Mission-critical Software)

---

# 3. 문토의 현재 품질 수준 및 변화 경로

## 컨설팅 이전: Level 1 (Ad-hoc)

| 항목 상태       |                                                                          |
| --------------- | ------------------------------------------------------------------------ |
| 스펙 문서 (SRS) | 없음. 요구사항은 구두나 메신저로 전달                                    |
| 이슈 관리       | 없음. Jira 미도입                                                        |
| 테스트 수행     | 개발자와 기획자가 수동으로 간단히 확인. 반복성 없음                      |
| QA 조직         | 없음                                                                     |
| CI/CD 자동 배포 | GitHub Actions 기반으로 일부 구성되어 있으나, 테스트 없이 단순 자동 배포 |
| 품질 관리 인식  | 문제 발생 시 사후 대응 중심. 계획적 관리 없음                            |

## 현재 (컨설팅 후 진행 중): Level 1.5

| 항목 상태 |  |
| --- | --- |
| 스펙 문서 (SRS) | 주요 기능 중심으로 작성 시작, 작성 문화 정착 중 |
| 이슈 관리 | Jira 도입 및 전사적 이슈 등록 체계화 |
| 테스트 수행 | 수동 테스트는 여전히 개발자/기획자/디자이너가 주도하며, 명확한 테스트 계획은 부재. 자동화 및 단위 테스트 도입은 논의 중이나 체계적 문서화는 미흡하여 반복성이 확보되지 않음 |
| QA 조직 | 미도입. 필요성 인식하고 있음 |
| CI/CD 자동 배포 | GitHub Actions 기반 자동 배포 안정화. 테스트 자동화로 확장 준비 중 |
| 품질 관리 인식 | 반복 가능한 프로세스를 갖추기 위한 기반 형성, 품질 메트릭 설정 준비 중 |

> 특히, 현재 상태는 테스트 수행 방식이 체계적이지 않고 테스트 계획이 문서화되어 있지 않으며, 대부분 개발자 또는 기획자가 주먹구구식으로 테스트를 수행하고 있습니다. 이로 인해 기능 누락이나 통합 환경 상의 문제를 사전에 발견하지 못하고, 릴리즈 직전 또는 이후에 긴급 대응하는 상황이 반복되고 있습니다. 이는 반복성과 재현성이 없다는 점에서 Level 2로 평가하기 어려우며, **Level 1.5** 상태로 정의하는 것이 타당합니다.

## 1차 목표: Level 3 (Defined)

Level 3 도달을 위해 필요한 핵심 과제는 다음과 같습니다:

- QA 담당자 또는 실무형 품질 리더 채용
- 테스트 케이스 및 시나리오 작성 체계화
- 유닛 테스트 및 E2E 테스트의 점진적 도입 및 자동화
- 정적 분석 도구 연계 및 커버리지 측정 도입
- 릴리즈 기준 수립 (예: 테스트 통과율, 코드 리뷰 완료 등)

Level 3는 소규모 기업도 충분히 도달 가능한 목표이며, 고객 신뢰도 제고와 내부 개발 효율 모두를 확보할 수 있는 중요한 전환점입니다.

---

# 4. 품질 체계 정비 전략

**조직 측면:**

- QA 담당자 또는 팀 신설 (초기에는 1명으로 시작 가능)
- 개발/기획/QA 간 협업 체계 정의 (예: QA는 테스트 계획 및 검증, 개발은 버그 수정)

**시스템 측면:**

- Test Case 및 Test Scenario를 관리할 수 있는 도구 도입 (예: TestRail, Xray for Jira)
- CI/CD 파이프라인에 테스트 자동화 단계 도입 (예: Playwright, Flutter integration test)
- **각 플랫폼별 단위 테스트 및 E2E 테스트 환경 구성 필요**

**프로세스 측면:**

- SRS → 개발 → 코드 리뷰 → QA 테스트 → 릴리즈 → 회고의 전체 사이클 정의
- 모든 기능에 대한 사전 테스트 계획 수립
- 이슈 관리(Jira)에 QA 검증 상태 연동

---

# 5. 향후 개선 로드맵

## AI 활용 전략 (Level 3 내 실용적 적용)

최근 AI 기술의 발달은 품질 관리 방식에도 혁신적인 기회를 제공하고 있습니다. 문토가 목표로 삼고 있는 Level 3 단계에서도, Level 5 수준에서만 활용된다고 여겨졌던 AI 기반 품질 관리 기법을 선택적·부분적으로 적용함으로써 비용 대비 높은 품질 효율성을 확보할 수 있습니다.

다음은 AI 기반 테스트 및 품질 관리 도구 정리 보고서에 기반하여, 문토에 실질적으로 적용 가능한 영역입니다:

### 1. 코드 생성 및 품질 보조

- **Cursor**: GPT 기반 코드 작성 도구로, 코드 자동 생성과 동시에 오류 탐지, 테스트 코드 제안, 리팩터링 등을 보조하여 실시간 품질 확보에 기여합니다.

### 2. 테스트 케이스 생성 보조

- **Testim Copilot**: 요구사항 기반 테스트 케이스 자동화
- **Reflect AI**: 실제 사용자 행동 기반 테스트 케이스 생성
- **Copilot for Test Cases**: ChatGPT 기반 자연어 테스트 시나리오 생성 GPT

### 3. PR 리뷰 시 품질 리스크 예측

- **Amazon CodeWhisperer**, **DeepCode**, **CodeAI**: PR 또는 코드 커밋 기준으로 AI가 보안·품질 리스크를 탐지 및 추천합니다.

### 4. 테스트 코드 자동화 초안 생성 (UI 중심)

- **mabl**, **Testim.io**, **Autify**, **Katalon TestOps AI**, **Virtuoso**: UI 테스트 시나리오를 자동 생성하고 유지보수하며, QA 비전문가도 사용할 수 있는 시각 기반 도구들입니다.

### 5. 버그 리포트 요약 및 이슈 등록 자동화 (Jira 연동)

- **Jira GPT Integration**, **Humata**, **Athenian**: 로그 및 사용자 피드백 기반 정보를 자동 분석해 Jira에 이슈를 자동 등록하거나 정리할 수 있습니다.

**장점:**

- 테스트 자동화 초기 단계에서도 QA 리소스를 줄이고 범위를 확대 가능
- 인력 효율성 확보 및 반복 테스트 업무 경감
- 품질 커버리지 확대 및 품질 개선 속도 가속화

문토와 같이 소규모 조직일수록 AI 기반 도구의 도입은 **QA 인력 부족을 보완하면서도** Level 3 수준의 품질을 한층 더 향상시키는 **전략적 선택**이 될 수 있습니다.

- **QA 인력 채용:** 최소 1명 채용하여 테스트 케이스 작성 및 검증 체계를 확립
- **자동화 테스트 기초 구축:** Flutter/E2E, Web(Cypress), Backend(Jest) 등 각 플랫폼별 자동화 테스트 시작
- **유닛 테스트 도입:** 핵심 로직에 대해 우선적으로 단위 테스트 작성 및 지속적 확대
- **E2E 테스트 구축:** 핵심 사용자 플로우에 대한 End-to-End 테스트 도입
- **릴리즈 프로세스 정비:** 모든 기능은 QA 승인 후 배포하도록 정책화
- **품질 메트릭 정의:** 버그 재발률, 릴리즈 안정성, 평균 해결 시간 등 수치 지표 설정

---

# 6. QA 조직 도입 효과

- **개발 생산성 향상:** QA가 사전 검증을 수행함으로써 개발자는 버그 대응 부담 감소
- **릴리즈 안정성 향상:** QA 확인을 통한 릴리즈 품질 확보
- **비용 절감:** 초기 테스트를 통한 재작업 감소로 인건비, 고객 CS 대응 비용 절감
- **고객 만족도 증가:** 버그 감소 및 안정적인 기능 제공
- **테스트 자동화의 효과:** 유닛 테스트 및 E2E 테스트를 통해 기능 회귀 테스트가 자동화되고, 릴리즈 속도와 정확도 모두 향상됨

---

# 7. 결론 및 제언

문토는 현재 Level 1.5 수준의 품질 관리 수준을 가지고 있으며, 이로 인해 여러 운영상 비효율이 발생하고 있습니다. 본 보고서에서 제시한 조직/시스템/프로세스 개선안을 순차적으로 도입함으로써, 품질 수준을 Level 3 이상으로 끌어올리고, 이를 통해 고객 만족도, 개발 효율성, 그리고 서비스 신뢰도를 함께 향상시킬 수 있습니다.

단순한 QA 인력 추가가 아닌, 전사적인 품질 문화의 정립이 장기적인 경쟁력으로 작용할 것입니다.

특히 소규모 조직이라는 이유로 QA 조직 신설을 주저하는 경향이 있으나, QA 전담 인력이 부재한 경우 개발자가 테스트 업무까지 병행해야 하며, 이는 기능 품질 저하와 일정 지연, 릴리즈 후 핫픽스 빈도 증가로 이어집니다. 결국 고객 신뢰 하락과 반복 작업으로 인한 인력 낭비가 발생합니다. 실제로 많은 소규모 기업들도 QA 전담 인력 1인을 먼저 채용해 체계를 만들고, 이후 자동화를 통해 품질과 비용을 동시에 관리하고 있습니다. 인원이 적은 조직일수록 QA 1명의 효과는 더욱 크며, 조직 전체의 운영 효율을 끌어올리는 중요한 투자임을 인식할 필요가 있습니다.

또한, 문토는 이미 GitHub Actions 기반의 CI/CD 자동 배포 시스템을 운영하고 있어 테스트 자동화 체계를 도입하기 위한 인프라는 충분히 갖춰져 있습니다. 적절한 품질 리더를 찾기 어렵다는 우려도 있지만, 초기에는 실무형 QA 1인을 두고 점진적으로 프로세스를 정비하며, 외부 컨설팅이나 커뮤니티 기반의 가이드를 통해 체계를 확립할 수 있습니다. 품질 리더의 완벽한 전문성이 아니라, 실행과 책임의 일관성이 핵심입니다.

---

# 부록: 용어 설명

- **SRS (Software Requirements Specification):** 소프트웨어가 무엇을 해야 하는지 정리한 문서. 개발 전 요구사항을 명확히 하여 혼선을 줄입니다.
- **WBS (Work Breakdown Structure):** 전체 업무를 작은 단위로 나누어 일정과 책임을 관리하기 위한 구조입니다.
- **Jira:** 개발 과정에서 발생하는 이슈나 작업을 관리하는 툴로, 프로젝트 관리 도구입니다.
- **QA (Quality Assurance):** 품질을 보증하기 위한 조직 또는 활동. 제품이 요구사항을 충족하는지 사전에 검증합니다.
- **유닛 테스트 (Unit Test):** 개발자가 작성한 코드 중 가장 작은 단위(함수 또는 메서드 등)가 올바르게 동작하는지 자동으로 검증하는 테스트입니다.
- **E2E 테스트 (End-to-End Test):** 사용자의 실제 사용 흐름을 그대로 시뮬레이션하여 전체 시스템이 정상 동작하는지 확인하는 테스트입니다.
- **CI/CD (Continuous Integration / Continuous Delivery):** 코드 변경을 자동으로 빌드하고 테스트하여 배포까지 연결하는 자동화 프로세스입니다.
- **테스트 커버리지:** 전체 코드 중 몇 %가 테스트되고 있는지를 수치로 나타내는 지표입니다. 커버리지가 높을수록 놓치는 버그가 줄어듭니다.
- **정적 분석:** 코드 실행 없이 코드 구조나 패턴을 분석하여 오류 가능성을 사전에 탐지하는 기법입니다.

소프트웨어 품질 관리 수준은 다음과 같이 레벨링 할 수 있습니다. 이는 CMMI(Capability Maturity Model Integration)나 TMMi(Test Maturity Model Integration) 등의 모델을 참고하여 일반화한 단계입니다.

- **Level 1 – 초기(Ad-hoc, 주먹구구)**
  - 프로세스가 정의되지 않음
  - 스펙 없음, 문서 없음, 기록 없음
  - 개발자 개인 역량에 의존
  - 테스트는 수동으로, 릴리즈 후 즉각적인 수정 중심
  - 예시:
    - 초기 스타트업 (Seed 단계)
    - 1~5인 조직, 기획 없이 바로 구현 → 배포 → 사용자 피드백 기반 수정
    - 예: 창업 직후의 사이드 프로젝트, 혹은 첫 번째 MVP만 빠르게 내놓는 소규모 팀
- **Level 2 – 반복 가능(Repeatable)**
  - SRS(요구사항 명세서)와 일정 관리(WBS) 존재
  - Jira 등 이슈 관리 시스템 도입
  - 기본적인 형상 관리 및 버전 관리 체계 존재
  - 테스트 계획은 일부 존재하나, 이를 전담하여 조직이 없음
  - 예시:
    - 초기 품질 체계를 갖춘 스타트업이 QA 조직 없이 프로세스를 정비해가는 과도기적 단계
    - 일정/이슈 관리는 Jira를 기반으로 문서화 및 반복성을 확보하는 중
    - 예: 시리즈 A~B 단계에서 성장을 준비하며 품질 체계 정립을 시작한 조직
    - QA 전담자는 없지만, 명확한 품질 목표를 세우고 테스트 자동화를 준비하는 10명 내외 규모의 기술 스타트업
- **Level 3 – 정의됨(Defined)**
  - QA 조직 혹은 전담 테스트 팀 존재
  - 테스트 케이스, 테스트 시나리오 체계화
  - 정적 분석 및 CI/CD 통합 테스트 자동화 일부 수행
  - 릴리즈 기준 정의 (ex. 버그 제로, 커버리지 등)
  - **유닛 테스트 및 E2E 테스트 일부 도입**
  - 예시:
    - 초기 토스, 당근마켓 등 성장을 준비하는 스타트업의 플랫폼 팀
    - 10~30인 규모의 제품 조직이 있는 중견 스타트업
